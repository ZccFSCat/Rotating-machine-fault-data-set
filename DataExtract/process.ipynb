{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N--Normal\n",
    "I--Imbalance\n",
    "\n",
    "S1--sensor1\n",
    "S2--sensors2\n",
    "\n",
    "H--Horizontal HORIZ\n",
    "V--Vertical VERT\n",
    "A--Axial AXIAL\n",
    "\n",
    "V--Velocity\n",
    "A--Acceleration \n",
    "AP--AccelerationP2P\n",
    "\n",
    "ex：NS1HV--Normal sensor1 Horizontal Velocity\n",
    "sp：Horizontal has no Acceleration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge folder files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "folder_path = '/Users/zhangcheng/DataspellProjects/ISCF/DataExtract/Rotating Machinery Repository - Faulty/Sensor 2/Vibration - Vertical/velocityRMS'\n",
    "\n",
    "# Initialise an empty DataFrame to store the data\n",
    "dfs = []\n",
    "\n",
    "\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.csv'):  \n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        \n",
    "        \n",
    "        df = pd.read_csv(file_path)\n",
    "        dfs.append(df)\n",
    "\n",
    "# Use the concat function to combine all data frames into one\n",
    "merged_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# \n",
    "output_file = '/Users/zhangcheng/DataspellProjects/ISCF/DataExtract/Rotating Machinery Repository - Faulty/Sensor 2/IS2VV.csv'\n",
    "\n",
    "# \n",
    "merged_df.to_csv(output_file, index=False)  # \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge data for each feature, 8 columns, AAP, AA, AV, HAP, HV, VAP, VA, VV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define the directory containing the CSV files\n",
    "directory_path = \"/Users/zhangcheng/DataspellProjects/ISCF/DataExtract/Normal/NoFault/Sensor2\"\n",
    "\n",
    "# Initialize an empty DataFrame for the combined data\n",
    "combined_columns_df = pd.DataFrame()\n",
    "\n",
    "# Loop through each file in the directory\n",
    "for file in os.listdir(directory_path):\n",
    "    # Check if the file is a CSV file\n",
    "    if file.endswith(\".csv\"):\n",
    "        file_path = os.path.join(directory_path, file)\n",
    "        \n",
    "        # Read the CSV file\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Check if the DataFrame has at least 2 columns\n",
    "        if df.shape[1] < 2:\n",
    "            continue  # Skip files with less than 2 columns\n",
    "        \n",
    "        # Extract the second column and rename it\n",
    "        second_col = df.iloc[:, 1]\n",
    "        second_col.name = file.replace('.csv', '')  # Use the file name (without .csv) as the column name\n",
    "        \n",
    "        # Add the second column to the combined DataFrame\n",
    "        combined_columns_df = pd.concat([combined_columns_df, second_col], axis=1)\n",
    "\n",
    "# Define the path for the new combined CSV file\n",
    "combined_file_path = \"/Users/zhangcheng/DataspellProjects/ISCF/DataExtract/Normal/NoFault/combined_N_S2.csv\"\n",
    "\n",
    "# Save the combined DataFrame to a new CSV file\n",
    "combined_columns_df.to_csv(combined_file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete data smaller than the mean value of the first column, simulate switching off the computer, and save the filtered data to a new CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered data has been saved to /Users/zhangcheng/DataspellProjects/ISCF/DataExtract/Normal/NoFault/filtered_N_S2.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the path to the original CSV file\n",
    "file_path = \"/Users/zhangcheng/DataspellProjects/ISCF/DataExtract/Normal/NoFault/combined_N_S2.csv\"\n",
    "\n",
    "# Reading CSV files\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Calculate the mean of the first column\n",
    "first_col_mean = df.iloc[:, 0].mean()\n",
    "\n",
    "# Filter out rows where the first column is greater than or equal to the mean value\n",
    "filtered_df = df[df.iloc[:, 0] >= first_col_mean]\n",
    "\n",
    "# Define the path to the new CSV file to be saved\n",
    "filtered_file_path = \"/Users/zhangcheng/DataspellProjects/ISCF/DataExtract/Normal/NoFault/filtered_N_S2.csv\"\n",
    "\n",
    "# Save filtered data to a new CSV file\n",
    "filtered_df.to_csv(filtered_file_path, index=False)\n",
    "\n",
    "print(f\"Filtered data has been saved to {filtered_file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine multiple documents according to the first column of time,Inconsistency in the timing of data collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "# File paths for the uploaded files\n",
    "file_paths_ns = {\n",
    "    \"NS2AA\": \"/Users/zhangcheng/DataspellProjects/ISCF/DataExtract/Normal/NoFault/Sensor2/Vibration - Axial/NS2AA.csv\",\n",
    "    \"NS2AAP\": \"/Users/zhangcheng/DataspellProjects/ISCF/DataExtract/Normal/NoFault/Sensor2/Vibration - Axial/NS2AAP.csv\",\n",
    "    \"NS2AV\": \"/Users/zhangcheng/DataspellProjects/ISCF/DataExtract/Normal/NoFault/Sensor2/Vibration - Axial/NS2AV.csv\",\n",
    "    \"NS2HAP\": \"/Users/zhangcheng/DataspellProjects/ISCF/DataExtract/Normal/NoFault/Sensor2/Vibration - Horizontal/NS2HAP.csv\",\n",
    "    \"NS2HV\": \"/Users/zhangcheng/DataspellProjects/ISCF/DataExtract/Normal/NoFault/Sensor2/Vibration - Horizontal/NS2HV.csv\",\n",
    "    \"NS2VA\": \"/Users/zhangcheng/DataspellProjects/ISCF/DataExtract/Normal/NoFault/Sensor2/Vibration - Vertical/NS2VA.csv\",\n",
    "    \"NS2VAP\": \"/Users/zhangcheng/DataspellProjects/ISCF/DataExtract/Normal/NoFault/Sensor2/Vibration - Vertical/NS2VAP.csv\",\n",
    "    \"NS2VV\": \"/Users/zhangcheng/DataspellProjects/ISCF/DataExtract/Normal/NoFault/Sensor2/Vibration - Vertical/NS2VV.csv\"\n",
    "}\n",
    "\n",
    "# Load and display the first few rows of each file to understand their structure\n",
    "data_samples_ns = {name: pd.read_csv(path).head() for name, path in file_paths_ns.items()}\n",
    "data_samples_ns\n",
    "\n",
    "\n",
    "# Convert 'date' column to datetime and merge all files based on the 'date' column\n",
    "data_frames_ns = [pd.read_csv(path).assign(date=lambda df: pd.to_datetime(df['date'], unit='ms')) for path in file_paths_ns.values()]\n",
    "\n",
    "# Merging all dataframes on the 'date' column\n",
    "merged_ns = data_frames_ns[0]\n",
    "for df in data_frames_ns[1:]:\n",
    "    merged_ns = pd.merge(merged_ns, df, on='date', how='outer')\n",
    "\n",
    "# Sorting by date and resetting the index\n",
    "merged_ns = merged_ns.sort_values('date').reset_index(drop=True)\n",
    "\n",
    "# Displaying the first few rows of the merged dataset\n",
    "merged_ns.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forward and Backward Filling Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward filling and then backward filling to minimize missing data as much as possible\n",
    "forward_filled_ns = merged_ns.fillna(method='ffill')\n",
    "backward_filled_ns = forward_filled_ns.fillna(method='bfill')\n",
    "\n",
    "# Displaying the first few rows of the filled dataset and checking for remaining missing data\n",
    "filled_ns_info = {\n",
    "    \"head\": backward_filled_ns.head(),\n",
    "    \"missing_data\": backward_filled_ns.isnull().sum()\n",
    "}\n",
    "filled_ns_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"/Users/zhangcheng/DataspellProjects/ISCF/DataExtract/Fault/Sensor 1/merged_sensor_data.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Using forward padding for missing data\n",
    "df_forward_filled = df.fillna(method='ffill')\n",
    "\n",
    "# Use backward padding to handle the remaining missing data\n",
    "df_filled = df_forward_filled.fillna(method='bfill')\n",
    "\n",
    "# Prints the first few rows of the populated data frame\n",
    "print(df_filled.head())\n",
    "\n",
    "# If desired, you can save the populated data as a new CSV file\n",
    "output_file_path = \"/Users/zhangcheng/DataspellProjects/ISCF/DataExtract/Fault/Sensor 1/filled_data.csv\"\n",
    "df_filled.to_csv(output_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove data below the mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the mean for each column\n",
    "means = backward_filled_ns.mean()\n",
    "\n",
    "# Removing rows where all values (excluding 'date') are below their respective column means\n",
    "filtered_ns = backward_filled_ns[~(backward_filled_ns.loc[:, \"NS1AA_rms\":] < means).all(axis=1)]\n",
    "\n",
    "# Displaying the first few rows of the filtered dataset and the new dataset shape\n",
    "filtered_ns_info = {\n",
    "    \"head\": filtered_ns.head(),\n",
    "    \"shape\": filtered_ns.shape\n",
    "}\n",
    "filtered_ns_info\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equal resampling intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the most common time interval in the original data\n",
    "time_diffs = sorted_filtered_ns['date'].diff().dt.total_seconds().mode()\n",
    "\n",
    "# If multiple modes are found, choose the smallest one as the interval\n",
    "interval = time_diffs.min()\n",
    "\n",
    "# Resampling the data to the determined interval\n",
    "# 'date' needs to be the index for resampling\n",
    "sorted_filtered_ns.set_index('date', inplace=True)\n",
    "resampled_ns = sorted_filtered_ns.resample(f'{int(interval)}S').mean().interpolate()\n",
    "\n",
    "# Resetting index to bring 'date' back as a column\n",
    "resampled_ns.reset_index(inplace=True)\n",
    "\n",
    "# Displaying the first few rows of the resampled dataset and the interval used\n",
    "resampled_info = {\n",
    "    \"head\": resampled_ns.head(),\n",
    "    \"interval_seconds\": interval\n",
    "}\n",
    "resampled_info\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "another"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's load and examine the data from the provided files to understand their structure and contents.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the files into dataframes\n",
    "files = [\n",
    "    \"/mnt/data/NS2AA.csv\", \"/mnt/data/NS2AAP.csv\", \"/mnt/data/NS2AV.csv\",\n",
    "    \"/mnt/data/NS2HAP.csv\", \"/mnt/data/NS2HV.csv\", \"/mnt/data/NS2VA.csv\",\n",
    "    \"/mnt/data/NS2VAP.csv\", \"/mnt/data/NS2VV.csv\"\n",
    "]\n",
    "\n",
    "dataframes = [pd.read_csv(file) for file in files]\n",
    "\n",
    "# Display the first few rows of each dataframe to understand their structure\n",
    "for i, df in enumerate(dataframes):\n",
    "    print(f\"Data from file {i+1} ({files[i]}):\")\n",
    "    print(df.head(), \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the timestamp from integer to datetime format\n",
    "for df in dataframes:\n",
    "    df['date'] = pd.to_datetime(df['date'], unit='ms')\n",
    "\n",
    "# Re-trying the merge with the converted datetime\n",
    "merged_df = dataframes[0]\n",
    "\n",
    "for df in dataframes[1:]:\n",
    "    merged_df = pd.merge_asof(merged_df.sort_values('date'), df.sort_values('date'), on='date', tolerance=pd.Timedelta('1min'), direction='nearest')\n",
    "\n",
    "# Renaming columns for clarity\n",
    "column_names = ['date', 'AA_rms', 'AAP_amplitude', 'AV_rms', 'HAP_amplitude', 'HV_rms', 'VA_rms', 'VAP_amplitude', 'VV_rms']\n",
    "merged_df.columns = column_names\n",
    "\n",
    "# Displaying the first few rows of the merged dataframe\n",
    "merged_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Excluding the 'date' column for mean calculation and filtering\n",
    "numeric_df = filled_df.drop(columns=['date'])\n",
    "\n",
    "# Calculating the mean for each numeric column\n",
    "means = numeric_df.mean()\n",
    "\n",
    "# Filtering out the rows where all values are below their respective means\n",
    "filtered_df = filled_df[(numeric_df > means).any(axis=1)]\n",
    "\n",
    "# Exporting the final filtered data to a new CSV file\n",
    "final_filtered_file_path = \"/mnt/data/final_filtered_sensor_data.csv\"\n",
    "filtered_df.to_csv(final_filtered_file_path, index=False)\n",
    "\n",
    "final_filtered_file_path\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
